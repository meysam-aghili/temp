{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from etl import *\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_mbf_file_paths(file_paths):\n",
    "    file_hash = pd.DataFrame([[file_path,get_file_hash(file_path)] for file_path in file_paths]).rename({0:'file_path',1:'hash'},axis=1)\n",
    "    file_hash['is_duplicated'] = file_hash['hash'].duplicated()\n",
    "    dupelicated_df = file_hash[file_hash['is_duplicated']==True].reset_index(drop=True)\n",
    "    if os.path.isfile(config['excel_file_dir']):\n",
    "        excel_df = pd.read_excel(config['excel_file_dir'])\n",
    "        excel_hash = excel_df['hash'].drop_duplicates().reset_index().rename({'index':'is_exists'},axis=1)\n",
    "        hash_df = file_hash.merge( excel_hash, how='left', on='hash',suffixes=('_left', '_right'))\n",
    "        hash_df['is_exists'] = hash_df['is_exists'].apply(lambda x: False if pd.isna(x) else True)\n",
    "        updated_df = hash_df[(hash_df['is_duplicated']==False) & (hash_df['is_exists']==False)].reset_index(drop=True)\n",
    "        not_updated_df = hash_df[(hash_df['is_duplicated']==False) & (hash_df['is_exists']==True)].reset_index(drop=True)\n",
    "        not_updated_excel_df = excel_df[excel_df['hash'].isin(not_updated_df['hash'])]\n",
    "        return dupelicated_df['file_path'].to_list(), updated_df['file_path'].to_list(), not_updated_excel_df\n",
    "    else:\n",
    "        return dupelicated_df['file_path'].to_list(), file_hash[file_hash['is_duplicated']==False]['file_path'].to_list(), pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"./config.json\"):\n",
    "    with open(\"./config.json\") as config_file:\n",
    "        config = json.load(config_file)\n",
    "file_paths = get_mbf_file_paths(\"./mbf_files\")\n",
    "dupelicated_file_paths, updated_file_paths, not_updated_excel_df = get_updated_mbf_file_paths(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./mbf_files\\\\اردیبهشت\\\\140202-100-کسری شجاعی.mbf']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1 processed.\n"
     ]
    }
   ],
   "source": [
    "general_dataframe = get_dataframes(updated_file_paths)\n",
    "general_dataframe = pd.concat([not_updated_excel_df, general_dataframe]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_dataframe['order_id'] = pd.factorize(general_dataframe['hash'])[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1==1:\n",
    "    file_hash = pd.DataFrame([[file_path,get_file_hash(file_path)] for file_path in file_paths]).rename({0:'file_path',1:'hash'},axis=1)\n",
    "    file_hash['is_duplicated'] = file_hash['hash'].duplicated()\n",
    "    dupelicated_df = file_hash[file_hash['is_duplicated']==True].reset_index(drop=True)\n",
    "    if os.path.isfile(config['excel_file_dir']):\n",
    "        excel_df = pd.read_excel(config['excel_file_dir'])\n",
    "        excel_hash = excel_df['hash'].drop_duplicates().reset_index().rename({'index':'is_exists'},axis=1)\n",
    "        hash_df = file_hash.merge( excel_hash, how='left', on='hash',suffixes=('_left', '_right'))\n",
    "        hash_df['is_exists'] = hash_df['is_exists'].apply(lambda x: False if pd.isna(x) else True)\n",
    "        updated_df = hash_df[(hash_df['is_duplicated']==False) & (hash_df['is_exists']==False)].reset_index(drop=True)\n",
    "        not_updated_df = hash_df[(hash_df['is_duplicated']==False) & (hash_df['is_exists']==True)].reset_index(drop=True)\n",
    "        not_updated_excel_df = excel_df[excel_df['hash'].isin(not_updated_df['hash'])]\n",
    "        deleted_df = excel_df.merge(file_hash, how='left', on='hash', suffixes=(None,'_y'))\n",
    "        deleted = deleted_df[pd.isna(deleted_df['file_path_y'])]['hash'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
